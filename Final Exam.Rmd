---
title: "Final Exam"
author: "Morgan Conklin Spangler"
date: "3/13/2019"
output: pdf_document
header-includes:
  - \usepackage{setspace}\doublespacing
  - \usepackage{ amssymb }
  - \usepackage{amsmath}
  - \usepackage{tikz}
  - \usepackage{pgfplots}
  - \usepackage{float}
  - \floatplacement{figure}{H}
  - \usepackage{ dsfont }
  - \usepackage{ mathrsfs }
  - \usepackage{ wasysym }
  - \usepackage{mathtools}
  - \usepackage{tcolorbox}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
  1. Consider the model\newline (1) $y_{it}=x_{it}'\beta+u_i+\epsilon_{it} \ \ \ i=1,...,N;t=1,...,T,$ where\newline $E(x_{it}u_{it})=E(x_{it}\epsilon_{it})+E(u_i\epsilon_{it})=0$  \ \ \ for all $i$ and $t$;\newline $E(u_iu_j)=\sigma_u^2$ \ \ \ if $i=j$\newline $=0$ \ \ \ otherwise.
      a) Show that if $x_{it}=x_i$ for all $t$, then the OLS and GLS estimates are the same.\begin{tcolorbox} Redefine the error term as $\eta_{it}=u_i+\epsilon_{it}$. If $x_{it}=x_i$ for all $t$, then the predicted values of the dependant variable will also be time-invariant. $$\hat{y}_{it}=x_{it}'\beta\Rightarrow \hat{y}_{it}=\hat{y}_i=x_{i}'\beta$$ and it must follow that the residuals are $$e_{it}=y_{it}-\hat{y}_{it}=x_{it}'\beta+\eta_{it}-x_{it}'\beta=\eta_{it}.$$ Now, we solve for OLS estimates by, as always, minimizing the sum of squared residuals. $$\min_{b_{OLS}} \sum_{i=1}^n e_{it}^2$$ and $$b_{OLS}=(X'X)^{-1}X'Y.$$\newline\end{tcolorbox}
      \begin{tcolorbox}Then, to find the GLS estimates, we have $x_{it}=x_i \ \forall t$, which essentially means $T=1$. Then, the remainder of our parameters and predicted values will be time invariant as well. $$\eta_{i}=\sum_t \eta_{it}=\eta_{it}$$ $$y_{i}=\sum_t y_{it}=y_{it}$$ and we use least squares method to find our estimator by minimizing the sum of squared residuals. $$\min_{b_{GLS}} \sum_{i=1}^n e_{i}^2$$ and $$b_{GLS}=(X'X)^{-1}X'Y.$$ Which, to reiterate, when $x_{it}=x_i$, is equivalent to the OLS estimates. $$\Rightarrow b_{GLS}=(X'X)^{-1}X'Y=b_{OLS}$$\end{tcolorbox}
      b) Show that if $x_{it}=x_i$ for all $t$, then the OLS estimator applied to (1) is equivalent to the between estimator, which amounts to OLS applied to group means. That is, between estimator is given by\newline $b_B=[\sum_{i=1}^N \bar{x}_i\bar{x}_i']^{-1}\sum_{i=1}^N\bar{x}_i\bar{y}_i$\newline where $\bar{x}_i=T^{-1}\sum_{t=1}^Tx_{it}$ and $\bar{y}_i=T^{-1}\sum_{t=1}^Ty_{it}$.\begin{tcolorbox}From previous I have that $x_{it}=x_i \ \forall  \Rightarrow x_{it}=\sum_t=1^T x_{it}=x_i\Rightarrow T=1$. So, $\bar{x}_i=T^{-1}\sum_{t=1}^Tx_{it}=x_{it}=x_i$ and $\bar{y}_i=T^{-1}\sum_{t=1}^Ty_{it}=y_{it}=y_i$. Then, it follows that the between estimator can be rewritten as $$b_B=[\sum_{i=1}^N x_ix'_i]^{-1}\sum_{i=1}^N x_iy_i=(X'X)^{-1}X'Y$$ Given that $\sum_{i=1}^N x_i=X$ and $\sum_{i=1}^N y_i=Y$. So, we have that the between estimator is equivalent to the OLS estimator.$$b_B=(X'X)^{-1}X'Y=b_{OLS}$$\end{tcolorbox}
      c) You have just shown that when all regressors are time-invariant, the between estimator is efficient. Explain why this is true.\begin{tcolorbox} Given that the GLS estimator is the most efficient, provided that $\Omega$ can be estimated (general assumption), and we have shown that $b_{OLS}=b_{GLS}$, it is implied that when all regressors are time-invariant, the OLS estimator is also efficient. Then, we additionally have shown that $b_B=b_{OLS}$, so we have additionally shown that the between estimator is efficient.\end{tcolorbox} \begin{tcolorbox}Shown in another way, the GLS estimator is found with $$\Omega=TT', \Omega^{-1}=P'P, P=T^{-1}$$ for some nonsingular matrix $T$ which is $n\times n$. $$\Rightarrow Py=PX\beta+Pe\Rightarrow y^*=X^*\beta+e^*$$ $$E(e^*e^*|X)=E(Pee'P'|X)=P\sigma^2\Omega P'=\sigma^2PTT'P'=\sigma^2I.$$ Thus, we establish that GLS is efficient and can be written $b_{GLS}=(X'\Omega^{-1}X)^{-1}X'\Omega^{-1}e$. It has already been established that $b_{GLS}=b_{OLS}=b_B$, therefore $(X'X)^{-1}X'e$ implies that $\Omega=1$ and OLS is efficient as well. Then, as the between estimator is equivalent to the OLS estimator, it is further establish that the between estimator is efficient as well.\end{tcolorbox}
  2. When you take the 2nd-year econometrics sequence, you will learn about many-instrument bias. In a nutshell, with too many instruments, you risk picking up the first stage part of the endogenous variation that you were trying to eliminate by 2SLS. The limiting special case illustrates this problem nicely: if the number of instruments equals the number of observations, then 2SLS is equal to OLS. Prove this limiting special case. \begin{tcolorbox}Suppose we have some general IV estimation model, $$y_i=x_i\beta + D_i\delta+\epsilon_i, \ \ i=1,...,n$$ such that $D_i$ is our problem regressor.\end{tcolorbox} \begin{tcolorbox}We have some instrument $Z_i$ which is correlated with $D_i$ but uncorrelated with $\epsilon_i$. Thus, we will use $Z_i$ as an instrument for $D_i$. Further, suppose the instrument $Z_i$ is $$Z_i=[z_{i1} \ z_{i2} \ ... \ z_{in}],$$ so we have $n$ instruments and $n$ observations. In the first stage, we estimate $D_i$, $$D_i=\alpha Z_i+x_ib+u_i$$ and get OLS parameter estimates for the $n$ instruments.$$\hat{D}_i=\hat{\alpha} Z_i+x_i\hat{b}\Rightarrow D_i=\hat{D}_i+u_i$$Then, for the second stage, we substitute $\hat{D}_i$ for $D_i$, $$y_i=x_i\beta + (\hat{D}_i+u_i)\delta+\epsilon_i\Rightarrow y_i=x_i\beta + \hat{D}_i\delta+u_i\delta+\epsilon_i$$ Then, this regression is estimated using OLS to generate parameter estimates, $$\hat{y}_i=x_i\hat{\beta}+\hat{D}_i\hat{\delta}.$$ This can be rewritten as, $$\hat{y}_i=x_i\hat{\beta}+(\hat{\alpha}Z_i+\hat{b}x_i)\hat{\delta}=x_i\hat{\beta}+\hat{\delta}\hat{\alpha}Z_i+\hat{\delta}\hat{b}x_i.$$ Then, given there are just as many instruments as there are observations of the dependant variable, there is no remaining variation in $\hat{y}_i$ after the first stage that would remain to be captured in $x_i\hat{\beta}$. $$\sum_{i=1}^n \hat{y}_i=\sum_{i=1}^n x_i\hat{\beta}+\hat{\delta}\hat{\alpha}Z_i+\hat{\delta}\hat{b}x_i=\sum_{i=1}^n\hat{\delta}\hat{\alpha}Z_i+\hat{\delta}\hat{b}x_i.$$ Thus, 2SLS regression is equivalent to the simple OLS regression of $$y_i=x_i\beta_1 + \beta_2Z_i+e_i$$\end{tcolorbox}
\begin{tcolorbox}  Intuitively, if you think of $Z_i$ as a dummy for if person $i$ received a treatment, then having $Z_i$ exist for $n$ individuals is the same as saying all individuals received a treatment, so that first stage is essentially an OLS regression on the effect of being treated. Then, in trying to find the effect of not receiving treatment in the second stage, there are no remaining individuals (i) for which a $Z_i$ doesn't exist, so there is no remaining variation with which to find any effect. Thus, the 2SLS is equivalent to OLS.\end{tcolorbox}
  3. Often, an analyst would like to estimate a model where the dependant variable is in logarithms, as in\newline $\ln y_i=\beta_1+\beta_2x_i+\epsilon_i$\newline but the dependant variable contains several values of $y_i=0$. Increasingly, she might be tempted to make use of the inverse hyperbolic sine transformation, estimating\newline (2) \ \ $\sinh^{-1}(y_i)=\beta_1+\beta_2x_i+\epsilon_i$\newline which is often expressed using the equality $\sinh^{-1}(y_i)=\ln [y_i+\sqrt{(y_i^2)}]$. The professed virtue of this transformation is that it is defined at zero.
      a) Show that for values of $y_i\approx 0$, the model in (2) behaves similarly to $$y_i=\beta_1+\beta_2x_1+\epsilon_i$$ whereas for large values of $y_i$, it behaves similarly to $$\ln y_i=\beta_1+\beta_2 x_i+\epsilon_i.$$ *Hint*: $\frac{d}{dz}\sinh^{-1}(z)=\frac{1}{\sqrt{1+z^2}}$. \begin{tcolorbox}In order to show that (2) behaves similarly to the respective models at varying levels of $y_i$, I will show that the slopes of the respective models converge as $y_i$ approaches $0$ and $\infty$, respectively, and, thus, the models will converge in their estimations.\newline First, to show that (2) converges to $y_i=\beta_1+\beta_2x_1+\epsilon_i$ as $y_i\rightarrow 0$. The slopes of these models will be the same if, as $y_i\rightarrow 0$, $\frac{d}{d y_i} \sinh^{-1}(y_i)\rightarrow \frac{d}{dy_i}y_i$. $\frac{d}{dy_i}y_i=1$, so it is only necessary to show that $\frac{d}{d y_i} \sinh^{-1}(y_i)\rightarrow 1$ as $y_i\rightarrow 0$. Knowing that $\frac{d}{d y_i} \sinh^{-1}(y_i)=\frac{1}{\sqrt{1+y_i^2}}$, it is obvious that as $y_i$ approaches zero, on either side of the number line (+/-), this derivative will converge to 1. Thus, the slopes of these models will converge as $y_i\rightarrow 0$ and we can be assured that the estimates will be similar for $y_i\approx 0$. Again, this will be the case for values of $y_i$ close to 0 on either side of the number line, as $y_i^2=-(y_i)^2$.\end{tcolorbox} \begin{tcolorbox}Then, to show that (2) converges to $\ln y_i=\beta_1+\beta_2x_1+\epsilon_i$ as $y_i\rightarrow \infty$. The slopes of these models will be the same if, as $y_i\rightarrow \infty$, $\frac{d}{d y_i} \sinh^{-1}(y_i)\rightarrow \frac{d}{dy_i}\ln y_i$. $\frac{d}{dy_i}\ln y_i=\frac{1}{y_i}$, so it is only necessary to show that $\frac{d}{d y_i} \sinh^{-1}(y_i)\rightarrow \frac{1}{y_i}$ as $y_i\rightarrow \infty$. Knowing that $\frac{d}{d y_i} \sinh^{-1}(y_i)=\frac{1}{\sqrt{1+y_i^2}}$, it is obvious that as $y_i$ grows larger, on either side of the number line (+/-), this derivative will converge to $\frac{1}{\sqrt{y_i}}$, as $1+y_i^2\rightarrow y_i$ as $y_i\rightarrow \infty$. Thus, the slopes of these models will converge as $y_i\rightarrow \infty$ and we can be assured that the estimates will be similar as $y_i$ grows increasingly large. Again, this will be the case for values of $y_i$ close to large in magnitude on either side of the number line, as $y_i^2=-(y_i)^2$.\end{tcolorbox}
      b) Suppose you had a sizeable share of observations for which $y_i=0$. Propose an alternative approach to modeling $y_i$ as a function of $x_i$ that would capture roughly the same non-linearity expressed in (2).\begin{tcolorbox} An alternative method to ensure the model remains defined at $y_i=0$ (so, not using $\ln y_i$), but capturing the non-linearity in the data is to simply use splines. Using a spline function ensures the model is piecewise linear and continuous, which makes for friendly comparitive statics. This would be an especially useful method if there are clear deliminations in the regressors. For instance, if we are modeling wealth or earnings and $x_i$ is the level of education person $i$ has, or some other parameter that can be similarly grouped or binned. Another way of framing this solution is to use dummy variables in the regressors, if possible, to account for the changes in slopes at different levels of $x_i$.\end{tcolorbox}