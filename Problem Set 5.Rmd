---
title: "Problem Set 5"
author: "Morgan Conklin Spangler"
date: "5/5/2019"
output: pdf_document
header-includes:
  - \usepackage{setspace}\onehalfspacing
  - \usepackage{tcolorbox}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(comment=NA, fig.width=3, fig.height=3, message=FALSE, echo=FALSE, strip.white = TRUE, prompt=FALSE, tidy=TRUE, warning = FALSE)
```


\parindent 0.7cm

# Part 1

## 1. 

  a) Given some conditional PDF $$f(y_i|x_i)=\frac{1}{\alpha x_i}\exp (\frac{-y_i}{\alpha x_i})$$ 
  the MLE estimation of $\alpha$ is $\alpha_{MLE}=\frac{\bar{y}}{\bar{x}}$.\newline\newline
  $f(y|x)=\Pi_{i=1}^N \frac{1}{\alpha x_i}\exp (\frac{-y_i}{\alpha x_i})$\newline
  $\Rightarrow\max_{\alpha}\ln f(y|x)=\sum_{i=1}^N(-\ln(\alpha)-\ln(x_i)-\frac{(-y_i)}{\alpha x_i})$\newline 
  $=-N\ln \alpha -N\ln\bar{x} +\frac{N\times N\bar{y}}{\alpha N\bar{x}}$\newline
  First Order Conditions (FOCs):\newline 
  $\frac{d \ln f}{d\alpha}=-\frac{N}{\alpha}+\frac{N\times N\bar{y}}{\alpha^2 N\bar{x}}=0$\newline
  $\Rightarrow \frac{N}{\alpha}=\frac{N\bar{y}}{\alpha^2\bar{x}}$\newline 
  $\Rightarrow \alpha_{MLE}=\frac{\bar{y}}{\bar{x}}$.
  b)^[http://stat.math.uregina.ca/~kozdron/Teaching/Regina/252Winter16/Handouts/ch5.pdf] Given $E[y_i|x_i]=\alpha x_i$ we know the $k$th sample moment condition is $\hat{\mu}_k=\frac{1}{N}\sum_{i=1}^N y_i^k$. Then, the population moment condition is $\mu_k=\int_0^\infty y^k f(y|x)dy$^[$E[\hat{\mu}_k]=E[\mu_k]$]. Then, we solve for the method of moments estimator $\hat\alpha_{MM}$ which is the $\alpha$ that solves $\hat{\mu}_k=\mu_k$.
  $$\mu_k=\int_0^\infty y^k f(y|x)dy\Rightarrow \mu_1=\frac{1}{\alpha x}\int_0^\infty y\exp (\frac{-y}{\alpha x})dy$$ $$\alpha x\mu_1=\int_0^\infty y\exp (\frac{-y}{\alpha x})dy=(\alpha x)(-\exp(\frac{-y}{\alpha x})(\alpha x+y))\vert_0^\infty$$ $$\Rightarrow \mu_1=(-\exp(\frac{-y}{\alpha x})(\alpha x+y))\vert_0^\infty=(-\exp(\frac{-\infty}{\alpha x})(\alpha x+\infty))-(-\exp(\frac{0}{\alpha x})(\alpha x))=-(-\alpha x)=\alpha x$$ $$\hat\mu_1=\frac{1}{N}\sum_{i=1}^N y_i=\bar{y}$$ Therefore, $\mu_1=\hat\mu_1$ when $\alpha$ is such that\newline $\bar{y}=\alpha x\Rightarrow \bar{y}=\alpha \bar x$^[Since this is valid for any $x$.]$$\Rightarrow \hat\alpha_{MM}=\frac{\bar y}{\bar x}$$
  c) Given a set of specific assumptions, the Cramer Rao Lower Bound theorem (CRLB) states that the MLE estimator achieves the minimum variance. This is because the method of moments estimator does not rely on these specific assumptions (correct model specification, for instance). However, I have found that the two estimators are equivalent given our knowledge of the distribution and the conditional expectation, thus the two have equivalent levels of variance.

## 2.

  a) $P_{ij}=Pr(U_{ij}>U_{im})$ for any $m\neq j$.\newline $\Rightarrow Pr(X'_{ij}\beta+\epsilon_{ij}>X'_{im}\beta+\epsilon_{im})=Pr((X'_{ij}-X'_{im})\beta+(\epsilon_{ij}-\epsilon_{im})>0|\bf{X})$
  b) The likelihood function is $L(\beta)=\Pi_{i=1}^N\Pi_{j=1}^J(P_{ij}^{y_{ij}})=\Pi_{i=1}^N\Pi_{j=1}^J(\frac{\exp(X'_{ij}\beta)}{\sum_{m=1,m\neq j}^J\exp(X'_{im}\beta)})^{y_{ij}}=\Pi_{i=1}^N(\frac{\exp(X'_{i1}\beta)}{\sum_{m=1,m\neq 1}^J\exp(X'_{im}\beta)})^{y_{i1}}\cdot\frac{\exp(X'_{i2}\beta)}{\sum_{m=1,m\neq 2}^J\exp(X'_{im}\beta)})^{y_{i2}}...\frac{\exp(X'_{iJ}\beta)}{\sum_{m=1,m\neq J}^J\exp(X'_{im}\beta)})^{y_{iJ}}$\newline $\Rightarrow L(\beta)=\Pi_{i=1}^N(\frac{\sum_{j=1}^J \exp(X'_{ij}\beta)}{\sum_{m=1,m\neq j}^J\exp(X'_{im}\beta)})^{\sum_{j=1}^J{y_{ij}}}$\newline $L(\beta)=(\frac{\sum_{j=1}^J \exp(X'_{1j}\beta)}{\sum_{m=1,m\neq j}^J\exp(X'_{1m}\beta)})^{\sum_{j=1}^J{y_{1j}}}\cdot (\frac{\sum_{j=1}^J \exp(X'_{2j}\beta)}{\sum_{m=1,m\neq j}^J\exp(X'_{2m}\beta)})^{\sum_{j=1}^J{y_{2j}}}...\cdot(\frac{\sum_{j=1}^J \exp(X'_{Nj}\beta)}{\sum_{m=1,m\neq j}^J\exp(X'_{Nm}\beta)})^{\sum_{j=1}^J{y_{Nj}}}$\newline $\Rightarrow L(\beta)=(\frac{\sum_{j=1}^J \exp(X'_{ij}\beta)}{\sum_{m=1,m\neq j}^J\exp(X'_{im}\beta)})^{\sum_{i=1}^N\sum_{j=1}^J{y_{ij}}}$\newline $\ln(L(\beta))=LL(\beta)=\ln(\frac{\sum_{j=1}^J \exp(X'_{ij}\beta)}{\sum_{m=1,m\neq j}^J\exp(X'_{im}\beta)})^{\sum_{i=1}^N\sum_{j=1}^J{y_{ij}}}=\sum_{i=1}^N\sum_{j=1}^J{y_{ij}}[\ln(\sum_{j=1}^J\exp(x'_{ij}\beta))-\ln(\sum_{m=1,m\neq j}^J\exp(X'_{im}\beta))]=\sum_{i=1}^N\sum_{j=1}^J{y_{ij}}\ln(\sum_{j=1}^J\exp(x'_{ij}\beta))-\sum_{i=1}^N\sum_{j=1}^J{y_{ij}}\ln(\sum_{m=1,m\neq j}^J\exp(X'_{im}\beta))$ $$LL(\beta)=\sum_{i=1}^N\sum_{j=1}^J{y_{ij}}\ln(\sum_{j=1}^J\exp(x'_{ij}\beta))-\sum_{i=1}^N\sum_{j=1}^J{y_{ij}}\ln(\sum_{m=1,m\neq j}^J\exp(X'_{im}\beta))$$
  c) The first order condition of the log-likelihood function is the derivative of $LL(\beta)$ with respect to $\beta$.\newline $LL(\beta)=\sum_{i=1}^N\sum_{j=1}^J{y_{ij}}\ln(\sum_{j=1}^J\exp(x'_{ij}\beta))-\sum_{i=1}^N\sum_{j=1}^J{y_{ij}}\ln(\sum_{m=1,m\neq j}^J\exp(X'_{im}\beta))$\newline FOC: $\frac{dLL(\beta)}{d\beta}=(\sum_{i=1}^N\sum_{j=1}^J{y_{ij}}\ln(\sum_{j=1}^J\exp(x'_{ij}\beta))-\sum_{i=1}^N\sum_{j=1}^J{y_{ij}}\ln(\sum_{m=1,m\neq j}^J\exp(X'_{im}\beta)))d\beta=(\sum_{i=1}^N\sum_{j=1}^J{y_{ij}}\ln(\sum_{j=1}^J\exp(x'_{ij}\beta))d\beta-(\sum_{i=1}^N\sum_{j=1}^J{y_{ij}}\ln(\sum_{m=1,m\neq j}^J\exp(X'_{im}\beta)))d\beta$
  d) The Independence of Irrelevant Alternatives assumption (IIA) is basically the assumption that my probabilities of preferring option A to option B does not change by the inclusion of other options. For instance, if I split my decision between choosing A and B 50/50, this should not change if C is also an option. This does not mean A and B must still be choosen 50% of the time each, just that the proportion should not change. So, I could choose C 100% of the time now, or 0%, or anywhere in between. It is only that $\frac{\Pr(A)}{\Pr(B)}=\frac{50}{50}=1$ cannot change. It is hard to think of examples which fit this scenario, but suppose I'm severely lactose intolerant. When given the option between vanilla and chocolate ice cream, I will be split evenly because either one will make me severely ill. Therefore, $\frac{\Pr(Ch)}{\Pr(Va)}=\frac{50}{50}=1$. Now, suppose mint ice cream is another flavor. I will still be evenly split among all of them as they will all still make me severely ill. Thus, the IIA assumption is satisfied.  $\Pr(Ch)=\frac{1}{3},\Pr(Va)=\frac{1}{3},\Pr(Mi)=\frac{1}{3}\Rightarrow \frac{\Pr(Ch)}{\Pr(Va)}=\frac{.33}{.33}=1$. Now, a dairy-free alternative is offered. This still maintains the IIA assumption as this new option would be choosen essentially 100% of the time, with the very small residual (due to $\epsilon$) split evenly among the other options.
  
## 3.

  a) First, given the moment conditions $E[X_i\cdot (T_i-\exp(\beta'x))]=0$ and $E[X_i\cdot (T_i^2-\exp(2\beta'x))]=0$, we derive the sample moment conditions, $\frac{1}{N}\sum_{i=1}^N[X_i\cdot (T_i-\exp(\beta'x))]=0$ and $\frac{1}{N}\sum_{i=1}^N[X_i\cdot (T_i^2-\exp(2\beta'x))]=0$. Using the two step GMM I would estimate $\beta$ by choosing some arbitrary starting point of my weighting matrix, $W$,^[which must be positive definite] to try and find the minimize the variance of the $\beta$. Then, we use this $W$ to solve the objective function. $$\hat\beta = \arg\min_{\beta}(\frac{1}{N}\sum_{i=1}^N\psi(X_i,T_i,\beta))'W(\frac{1}{N}\sum_{i=1}^N\psi(X_i,T_i,\beta))$$ where $\psi(X_i,T_i,\beta)=(\begin{pmatrix} \frac{1}{N}\sum_{i=1}^N X_i\cdot (T_i-\exp(\beta'x))\\ \frac{1}{N}\sum_{i=1}^N X_i\cdot (T_i^2-\exp(2\beta'x)) \end{pmatrix}$.\newline With this arbitrary $W=\begin{pmatrix} w_{11} \ w_{12} \\ w_{21} \ w_{22} \end{pmatrix}$ which is positive definite, we do the matrix algebra to simplify and take the FOCs and solve to find $\hat{\beta}$.
  b) The asymptotic variance of this estimator is $$V[\hat\beta]=\frac{1}{N}(\frac{1}{N}\sum_{i=1}^N\frac{\partial \psi(X_i,T_i,\hat\beta)}{\partial \beta})(\frac{1}{N}\sum_{i=1}^N\psi(X_i,T_i,\hat\beta)\psi(X_i,T_i,\hat\beta)')^{-1}(\frac{1}{N}\sum_{i=1}^N\frac{\partial \psi(X_i,T_i,\hat\beta)}{\partial \beta})^{-1}$$ where, again, $\psi(X_i,T_i,\beta)=(\begin{pmatrix} \frac{1}{N}\sum_{i=1}^N X_i\cdot (T_i-\exp(\beta'x))\\ \frac{1}{N}\sum_{i=1}^N X_i\cdot (T_i^2-\exp(2\beta'x)) \end{pmatrix}$.
  c) Given that a method of moments estimator is equivalent to the MLE estimator, we can use the CRLB to achieve a minimum variance estimator where the log likelihood function of a single $i$ is computed and the variance is $N\times Var(LL(\cdot))$ for the single $i$.
  
## 4.

  a) The sample moment conditions from these population moment conditions are $(\begin{pmatrix}\sum_{i=1}^N(3x_i-\theta)\\\sum_{i=1}^N(12y_i-\theta) \end{pmatrix}=0$.
  b)The GMM objective function is $$Q_W{\theta} = \arg\min_{\theta}(\frac{1}{N}\sum_{i=1}^N\psi(x_i,y_i,\theta))'\bf{I}(\frac{1}{N}\sum_{i=1}^N\psi(x_i,y_i,\theta))$$ where $\psi(x_i,y_i,\theta)=(\begin{pmatrix} \sum_{i=1}^N(3x_i-\theta)\\ \sum_{i=1}^N(12y_i-\theta)\end{pmatrix}$ and $\bf{I}$ $= \begin{pmatrix} 1 & 0 \\ 0 & 1\end{pmatrix}$ $\Rightarrow Q_W{\theta} = \arg\min_{\theta} \frac{1}{N} \sum_{i=1}^N \begin{pmatrix} \sum_{i=1}^N(3x_i-\theta)\\ \sum_{i=1}^N(12y_i-\theta) \end{pmatrix}'\begin{pmatrix} 1\ 0\\ 0\ 1\end{pmatrix}\frac{1}{N}\sum_{i=1}^N\begin{pmatrix}\sum_{i=1}^N(3x_i-\theta)\\\sum_{i=1}^N(12y_i-\theta)\end{pmatrix}$. This becomes, $Q_W{\theta}=\begin{pmatrix}3\bar{x}-\theta\\12\bar{y}-\theta\end{pmatrix}'\begin{pmatrix} 1\ 0\\ 0\ 1\end{pmatrix}\begin{pmatrix}3\bar{x}-\theta\\12\bar{y}-\theta\end{pmatrix}$
  c) By solving $Q_W(\theta)$ we get,\newline $(\begin{pmatrix}3\bar{x}-\theta\\12\bar{y}-\theta\end{pmatrix}'\begin{pmatrix} 1\ 0\\ 0\ 1\end{pmatrix}\begin{pmatrix}3\bar{x}-\theta\\12\bar{y}-\theta\end{pmatrix}$ $\Rightarrow \begin{pmatrix}3\bar{x}-\theta,\ 12\bar{y}-\theta\end{pmatrix}\begin{pmatrix}3\bar{x}-\theta\\12\bar{y}-\theta\end{pmatrix}$ $\Rightarrow(3\bar{x}-\theta)^2- (12\bar{y}-\theta)^2$ $\Rightarrow 9(\bar x^2)+144(\bar y^2)-6\bar x \theta-24\bar y\theta+2(\theta^2)$\newline FOC:\newline $d\theta$: $-6\bar x-24\bar y +4\theta=0$ $$\Rightarrow \frac{3}{2}\bar x + 6\bar y=\hat\theta_{GMM}$$
  d) $\hat\theta_{GMM}=\lambda \bar{x}+(1-\lambda)\bar{y}\Rightarrow \frac{3}{2}\bar x + 6\bar y=\lambda \bar{x}+(1-\lambda)\bar{y}$.\newline $\frac{3}{2}\bar x + 5\bar y=\lambda \bar{x}-\lambda\bar{y}\Rightarrow \frac{3}{2}\bar x + 5\bar y=\lambda (\bar{x}-\bar{y})\Rightarrow \frac{3\bar x + 10\bar y}{2 (\bar{x}-\bar{y})}=\lambda$ The variance of $\hat\theta_{GMM}$ is $\lambda^2\sigma_{xx}+(1-\lambda)^2\sigma_{yy}+2\lambda\sigma_{xy}$. Given $x$ and $y$ are independent, $\sigma_{xy}=0$. Then, we are given $\sigma_{xx}=\sigma_{yy}=1$, therefore the variance is $\lambda^2\sigma_{xx}+(1-\lambda)^2\sigma_{yy}=\lambda^2+(1-\lambda)^2=\lambda^2+1-2\lambda-\lambda^2=1-2\lambda$ such that $\lambda=\frac{3\bar x + 10\bar y}{2 (\bar{x}-\bar{y})}$.\newline $1-2\frac{3\bar x + 10\bar y}{2 (\bar{x}-\bar{y})}=\frac{(\bar{x}-\bar{y})}{(\bar{x}-\bar{y})}-\frac{3\bar x + 10\bar y}{(\bar{x}-\bar{y})}=\frac{(\bar{x}-\bar{y}-3\bar x - 10\bar y)}{(\bar{x}-\bar{y})}=-\frac{(2\bar{x}+11\bar{y})}{(\bar{x}-\bar{y})}=Var({\hat{\theta}_{GMM})}$
  e) Given the optimal weighting matrix is $W^*=\alpha\cdot\begin{pmatrix}\sigma_{yy}\ -\sigma_{xy}\\-\sigma_{yx}\ \sigma_{xx}\end{pmatrix}$ and we want to fit this to $M=\begin{pmatrix}c \ 0 \\ 0 \ 1\end{pmatrix}$. $M=W^*$ when $\alpha=1$ and $c=\sigma_{yy}=1$. Therefore, the optimal weighting matrix is the identity matrix such that $c=1$ $\Rightarrow M=\begin{pmatrix}1 \ 0 \\ 0 \ 1\end{pmatrix}$.

# Part 2

## 1. 

  a)
    i.
    Comparing the average pre-period logged fatalities per capita of TU and control:
```{r}
library(foreign)
library(haven)
library(sandwich)
library(lmtest)
library(naniar)
library(dplyr)
library(cobalt)
library(stringr)
library(stringi)
library(tidyr)
library(naniar)
library(ggplot2)
library(reshape2)
library(ModelMetrics)
library(psych)



dat <- read_dta("traffic_safety_v3_v12.dta")
View(dat)
dat$lfatal <- log(dat$fatalities)
dat$lfatal_percap <- dat$lfatal/dat$population
treat <- subset(dat,dat$state==99)
control <- subset(dat,dat$state!=99)
control <- subset(control,control$state!=6)
control <- subset(control,control$state!=10)
control <- subset(control,control$state!=30)
control <- subset(control,control$state!=41)
treat$t<-1
control$t<-0

treat_pre <- subset(treat,treat$primary==0)
treat_post <- subset(treat,treat$primary==1)
avgfatal_treatpre <- mean(treat_pre$lfatal_percap)
avgfatal_control <- mean(control$lfatal_percap)
print("TU")
avgfatal_treatpre
print("Control")
avgfatal_control
```
```{r, fig.caption="Pre-Period Logged Traffic Fatalities by Year, Treatment Against Average of Control",echo=FALSE, out.width = '100%'}
knitr::include_graphics("Treatment v control over time.png")
```
    ii. Calculating the MSE for the pre-period
    I define the "best match" for the TU as the individual control state which has the minimum MSE for the pre-period, state 8 with MSE of 285990.1.
```{r}
ctrl_states <- unique(control$state)
mat<-matrix(data=NA, nrow=43, ncol=5)
i=1
t=1981 
iter=1
for(i in ctrl_states){
  a<-mean(subset(treat$fatalities,treat$year==t))
  b<-mean(subset(control$fatalities,control$year==t & control$state==i))
  mat[iter,1]=mse(a,b)
  i=i+1
  iter=iter+1
}
i=1
t=1982
iter=1
for(i in ctrl_states){
  a<-mean(subset(treat$fatalities,treat$year==t))
  b<-mean(subset(control$fatalities,control$year==t & control$state==i))
  mat[iter,2]=mse(a,b)
  i=i+1
  iter=iter+1
}
i=1
t=1983
iter=1
for(i in ctrl_states){
  a<-mean(subset(treat$fatalities,treat$year==t))
  b<-mean(subset(control$fatalities,control$year==t & control$state==i))
  mat[iter,3]=mse(a,b)
  i=i+1
  iter=iter+1
}
i=1
t=1984 
iter=1
for(i in ctrl_states){
  a<-mean(subset(treat$fatalities,treat$year==t))
  b<-mean(subset(control$fatalities,control$year==t & control$state==i))
  mat[iter,4]=mse(a,b)
  i=i+1
  iter=iter+1
}
i=1
t=1985 
iter=1
for(i in ctrl_states){
  a<-mean(subset(treat$fatalities,treat$year==t))
  b<-mean(subset(control$fatalities,control$year==t & control$state==i))
  mat[iter,5]=mse(a,b)
  i=i+1
  iter=iter+1
}

dfmse <- rowSums(mat)
df<-cbind(dfmse,ctrl_states)
opt_mse <- min(dfmse)
opt <- subset(df,dfmse==opt_mse)

opt
```
  b)
    i. The synthetic control method is useful when attempting to estimate a treatment effect in cases where there is not one control group (there could be multiple control groups) which meets the parallel trends assumption. This method allows you to weight over all potential control entities in order to most closely match a parallel pre-trend. The main drawback with this method is that it is unlikely these weights will result in a convincing match on unobservables, so you will never truly be able to have the parallel control assumption met.
    ii. With the synthetic control command provided, the program is fitting a linear model to best match the pre-trend line of the treatment group. The model will be optimized to minimize the mean squared error between the weighted combination of control states, to become the synthetic control state, to the treated unit. The model takes the shape of $$y_t=\alpha+\beta_1{College}_t+\beta_2{Beer}_t+\beta_3{Unemploy}_t+\beta_4{TotalVMT}_t+\beta_5{Precip}_t+\beta_6{Snow}_t+\epsilon.$$
    I did not, honestly, have a very sophisticated way of determining this specification. I believe these covariates are varied across all states and will likely influence fatalities. Also, a linear model seemed reasonable and is the easiest to interpret. In this specification, as in the previous, the dependant variable is logged fatalities per capita.
      
>>dat<-as.data.frame(dat)
  dataprep.out<-
  dataprep(
    foo = dat,
    predictors = c("college","beer","unemploy","totalvmt","precip","snow32"),
    predictors.op = c("mean"),
    dependent = "lfatal_percap",
    unit.variable = "state",
    time.variable = "year",
    treatment.identifier = 99,
    controls.identifier = c(1,2,3,4,5,7,8,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,31,32,33,34,35,36,37,38,39,40,42,43,44,45,46,47,48),
    time.predictors.prior = c(1981:1985),
    time.optimize.ssr = c(1981:1985),
    time.plot = 1981:2003
  )
synth.out <- synth(dataprep.out)

  c)
    i.
```{r,echo=FALSE, out.width = '100%'}
knitr::include_graphics("Gaps: Treated-Synthetic.png")
knitr::include_graphics("(path.plot), treated synthetic.png")
```
    ii. As the trend over time does not deviate from the trend of the synthetic control group, I do not conclude that this treatment is significant.

  d) In the previous problem set I found the treatment to be insignificant, so there does not appear to be a drastic difference.




